# Estrat√©gia 3: Paraleliza√ß√£o por Entidade ‚≠ê RECOMENDADO

**Status**: ‚úÖ Recomendado para implementa√ß√£o
**Complexidade**: ‚≠ê‚≠ê Baixa-M√©dia
**Risco**: ‚≠ê Muito Baixo
**Investimento**: 2-3h implementa√ß√£o + testes
**Probabilidade de Sucesso**: ~100%

---

## üìã Descri√ß√£o

Paralelizar a extra√ß√£o distribuindo **entidades completas** entre m√∫ltiplos processos independentes, ao inv√©s de dividir p√°ginas de uma mesma entidade.

**Princ√≠pio**: Cada processo extrai entidades diferentes do in√≠cio ao fim, de forma completamente isolada.

**Exemplo pr√°tico**:
```
Processo 1: Estado RJ (isolado)                   ‚Üí 8h
Processo 2: Petr√≥polis, S√£o Gon√ßalo, Volta R.     ‚Üí 2.5h
Processo 3: 18 entidades m√©dias (ESPECIAL)        ‚Üí 2h
Processo 4: 19 entidades pequenas (ESPECIAL)      ‚Üí 1h
Processo 5: 56 entidades (GERAL completo)         ‚Üí 2h

Tempo Total: ~8h (limitado pelo Estado RJ)
```

---

## üéØ Por Que Esta Estrat√©gia?

### Vantagens sobre Estrat√©gia 2 (API Ranges)

| Aspecto | Estrat√©gia 2 (API) | Estrat√©gia 3 (Entidades) |
|---------|-------------------|--------------------------|
| **Requer API** | ‚úÖ SIM (n√£o encontrada) | ‚ùå N√ÉO |
| **Investiga√ß√£o** | 3-4h | 0h (an√°lise j√° feita) |
| **Probabilidade Sucesso** | ~20% | ~100% |
| **Complexidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muito Alta | ‚≠ê‚≠ê Baixa-M√©dia |
| **Risco Legal** | ‚ö†Ô∏è Eng. reversa de API | ‚úÖ Mesmo m√©todo atual |
| **Implementa√ß√£o** | 4-6h (SE API existir) | 2-3h (garantido) |
| **ROI** | Negativo (-90%) | Positivo (+200%) |

**Conclus√£o**: Estrat√©gia 3 √© vi√°vel, segura e implement√°vel imediatamente.

---

## ‚öôÔ∏è Arquitetura Proposta

### Distribui√ß√£o de Entidades (VPS KVM 4 - 5 Processos)

#### Processo 1: Estado RJ (Isolado) - GARGALO INEVIT√ÅVEL
```
Entidade: Estado do Rio de Janeiro (Regime ESPECIAL)
Registros: 17,663
P√°ginas: ~1,767
Tempo Estimado: 7h 52min

Justificativa: Isolar para n√£o bloquear outros processos
```

#### Processo 2: Entidades Grandes (ESPECIAL)
```
Entidades:
  1. Petr√≥polis            ‚Üí 2,921 registros (1h 18min)
  2. S√£o Gon√ßalo           ‚Üí 1,423 registros (38min)
  3. Volta Redonda         ‚Üí   983 registros (26min)

Total: 5,327 registros (~2.5h)
```

#### Processo 3: Entidades M√©dias (ESPECIAL)
```
Entidades (18 entidades de 100-500 registros):
  - Maca√©, Barra Mansa, Campos dos Goytacazes
  - Teres√≥polis, Nova Igua√ßu, Belford Roxo
  - Itabora√≠, Duque de Caxias, S√£o Jo√£o de Meriti
  - Mag√©, Queimados, Angra dos Reis
  - Cabo Frio, Mesquita, Niter√≥i
  - Nova Friburgo, Resende, Nil√≥polis

Total: ~3,500 registros (~2h)
```

#### Processo 4: Entidades Pequenas (ESPECIAL)
```
Entidades (19 entidades de 0-100 registros):
  - Munic√≠pio do Rio de Janeiro, Maric√°, Itagua√≠
  - Japeri, Paracambi, Serop√©dica
  - Guapimirim, Tangu√°, S√£o Jos√© do Vale do Rio Preto
  - Areal, Comendador Levy Gasparian, Engenheiro Paulo de Frontin
  - Sapucaia, Carmo, Mendes
  - Para√≠ba do Sul, Pira√≠, Rio Claro
  - Bom Jardim

Total: ~800 registros (~1h)
```

#### Processo 5: Regime GERAL Completo
```
Entidades: Todas as 56 entidades do Regime GERAL
  - Munic√≠pio do Rio de Janeiro: 2,486 registros
  - INSS: 907 registros
  - Niter√≥i: 620 registros
  - S√£o Francisco de Itabapoana: 337 registros
  - Nova Igua√ßu: 245 registros
  - Outras 51 entidades: ~850 registros

Total: ~5,444 registros (~2h)
```

---

## üìä Distribui√ß√£o Balanceada por Tempo

### An√°lise de Balanceamento

```
Processo 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 8h (Estado RJ)
Processo 2: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             2.5h (3 grandes)
Processo 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               2h (18 m√©dias)
Processo 4: ‚ñà‚ñà‚ñà‚ñà‚ñà                                    1h (19 pequenas)
Processo 5: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                               2h (GERAL 56)

LIMITANTE: Processo 1 (Estado RJ)
```

**Efici√™ncia de Balanceamento**:
- Processo mais longo: 8h (Estado RJ)
- Processo mais curto: 1h (Pequenas)
- Diferen√ßa: 7h (87.5% de ociosidade para Processo 4)

**Imposs√≠vel balancear melhor porque**:
- Estado RJ n√£o pode ser dividido (sem API de pagina√ß√£o)
- Estado RJ representa 68% dos dados de ESPECIAL
- Gargalo arquitetural inevit√°vel

---

## üíª Implementa√ß√£o

### Arquitetura de C√≥digo

```
v2_parallel/
‚îú‚îÄ‚îÄ main_parallel.py              # Orquestrador principal
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ entity_groups.py          # Configura√ß√£o de grupos
‚îÇ   ‚îî‚îÄ‚îÄ process_config.py         # Config de processos
‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îî‚îÄ‚îÄ entity_extractor.py       # Extrator por entidade
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ process_manager.py        # Gerenciamento de processos
‚îÇ   ‚îî‚îÄ‚îÄ csv_merger.py             # Merge de CSVs parciais
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ process_*.log             # Logs por processo
```

### Pseudo-c√≥digo Principal

```python
# main_parallel.py

from multiprocessing import Pool, Manager
from config.entity_groups import PROCESS_GROUPS

def extract_entity_group(group_id, entities, regime):
    """
    Extrai um grupo de entidades em um processo separado

    Args:
        group_id: ID do processo (1-5)
        entities: Lista de EntidadeDevedora para este grupo
        regime: 'geral' ou 'especial'

    Returns:
        CSV path with extracted data
    """
    # Cada processo tem seu pr√≥prio browser
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        scraper = TJRJPrecatoriosScraper()
        all_precatorios = []

        for entidade in entities:
            logger.info(f"[P{group_id}] Extracting {entidade.nome_entidade}...")

            try:
                precatorios = scraper.get_precatorios_entidade(page, entidade)
                all_precatorios.extend(precatorios)
                logger.info(f"[P{group_id}] ‚úÖ {len(precatorios)} records extracted")

            except Exception as e:
                logger.error(f"[P{group_id}] ‚ùå Failed: {e}")
                # Continua para pr√≥xima entidade (resili√™ncia)

        # Salvar CSV parcial
        csv_path = f"data/partial/process_{group_id}_{regime}.csv"
        save_to_csv(all_precatorios, csv_path)

        browser.close()
        return csv_path

def run_parallel_extraction():
    """Executa extra√ß√£o paralela usando 5 processos"""

    # Definir grupos de entidades
    groups = [
        (1, [estado_rj_entidade], 'especial'),          # Isolado
        (2, GRANDES_ESPECIAL, 'especial'),              # 3 entidades
        (3, MEDIAS_ESPECIAL, 'especial'),               # 18 entidades
        (4, PEQUENAS_ESPECIAL, 'especial'),             # 19 entidades
        (5, ALL_GERAL, 'geral'),                        # 56 entidades
    ]

    # Executar em paralelo
    with Pool(processes=5) as pool:
        results = pool.starmap(extract_entity_group, groups)

    # Merge CSVs
    merge_csv_files(results, 'data/processed/precatorios_complete.csv')

    logger.info("‚úÖ Extraction complete!")

if __name__ == "__main__":
    run_parallel_extraction()
```

### Configura√ß√£o de Grupos

```python
# config/entity_groups.py

# Processo 1: Estado RJ (isolado)
ESTADO_RJ = EntidadeDevedora(
    id_entidade=1,
    nome_entidade="Estado do Rio de Janeiro",
    regime="especial",
    precatorios_pendentes=17663
)

# Processo 2: 3 grandes (ESPECIAL)
GRANDES_ESPECIAL = [
    EntidadeDevedora(id_entidade=2, nome_entidade="Petr√≥polis", ...),
    EntidadeDevedora(id_entidade=3, nome_entidade="S√£o Gon√ßalo", ...),
    EntidadeDevedora(id_entidade=4, nome_entidade="Volta Redonda", ...),
]

# Processo 3: 18 m√©dias (ESPECIAL)
MEDIAS_ESPECIAL = [
    EntidadeDevedora(id_entidade=5, nome_entidade="Maca√©", ...),
    EntidadeDevedora(id_entidade=6, nome_entidade="Barra Mansa", ...),
    # ... (16 mais)
]

# Processo 4: 19 pequenas (ESPECIAL)
PEQUENAS_ESPECIAL = [
    EntidadeDevedora(id_entidade=20, nome_entidade="Munic√≠pio RJ", ...),
    EntidadeDevedora(id_entidade=21, nome_entidade="Maric√°", ...),
    # ... (17 mais)
]

# Processo 5: GERAL completo
ALL_GERAL = [
    EntidadeDevedora(id_entidade=100, nome_entidade="Munic√≠pio RJ", regime="geral", ...),
    # ... (55 mais)
]
```

---

## üîß Implementa√ß√£o Detalhada

### Fase 1: Refatora√ß√£o (1h)

**Mudan√ßas necess√°rias**:
1. Extrair l√≥gica de extra√ß√£o de entidade para fun√ß√£o isolada
2. Aceitar lista de entidades como input
3. Retornar CSV path ao inv√©s de printar
4. Logs prefixados com ID do processo (`[P1]`, `[P2]`, etc)

**Arquivos a modificar**:
- `src/scraper.py`: Adicionar `extract_entities(entity_list)` method
- Criar `utils/process_manager.py`: Gerenciamento de processos

### Fase 2: Configura√ß√£o de Grupos (30min)

**Tarefas**:
1. Carregar entidades de `data/processed/entidades_*.csv`
2. Classificar por volume (grande, m√©dia, pequena)
3. Criar distribui√ß√£o balanceada
4. Validar que todas as 97 entidades est√£o inclu√≠das

**Valida√ß√£o**:
```python
# Garantir que nenhuma entidade foi esquecida
all_entities = set()
for group in [ESTADO_RJ, GRANDES, MEDIAS, PEQUENAS, ALL_GERAL]:
    all_entities.update([e.id_entidade for e in group])

assert len(all_entities) == 97, "Missing entities!"
```

### Fase 3: Orquestra√ß√£o Paralela (30min)

**Implementa√ß√£o**:
```python
from multiprocessing import Pool

def run_parallel():
    groups = prepare_entity_groups()

    with Pool(processes=5) as pool:
        csv_paths = pool.starmap(extract_entity_group, groups)

    # Merge CSVs
    final_csv = merge_csv_files(csv_paths)

    return final_csv
```

**Configura√ß√£o de Pool**:
- `processes=5`: N√∫mero de processos paralelos
- `maxtasksperchild=1`: Evitar memory leaks
- Cada processo spawn pr√≥prio browser

### Fase 4: Merge de CSVs (30min)

**L√≥gica**:
```python
def merge_csv_files(csv_paths, output_path):
    """Merge multiple CSV files into one"""

    all_dataframes = []

    for csv_path in csv_paths:
        df = pd.read_csv(csv_path)
        all_dataframes.append(df)

    # Concatenate
    merged = pd.concat(all_dataframes, ignore_index=True)

    # Validate columns
    assert len(merged.columns) == 19, "Missing columns!"

    # Sort by entidade + data_inscricao
    merged = merged.sort_values(['nome_entidade', 'data_inscricao'])

    # Save
    merged.to_csv(output_path, index=False)

    return output_path
```

### Fase 5: Testes (30min)

**Testes necess√°rios**:
1. Teste com 3 entidades pequenas (valida√ß√£o r√°pida)
2. Teste com 2 processos (GERAL + 1 entidade ESPECIAL)
3. Teste completo com 5 processos

**Script de teste**:
```python
# tests/test_parallel_extraction.py

def test_small_parallel():
    """Test with 3 small entities across 2 processes"""
    groups = [
        (1, [entity1, entity2], 'especial'),
        (2, [entity3], 'geral'),
    ]

    with Pool(2) as pool:
        results = pool.starmap(extract_entity_group, groups)

    # Validate CSVs exist
    assert all(os.path.exists(csv) for csv in results)

    # Validate data
    merged = merge_csv_files(results)
    df = pd.read_csv(merged)

    assert len(df) > 0, "No data extracted"
    assert len(df.columns) == 19, "Missing columns"
```

---

## üìä Ganhos Esperados

### Performance

| M√©trica | V1 (Atual) | V2 (Paralelo) | Melhoria |
|---------|------------|---------------|----------|
| **Tempo Total** | ~8h | ~8h | 0% ‚ö†Ô∏è |
| **CPU Utilizado** | 50% (2 cores) | 90-100% (4 cores) | +80% ‚úÖ |
| **Resili√™ncia** | Baixa | Alta | +200% ‚úÖ |
| **Re-execu√ß√£o** | Tudo do zero | Apenas entidades falhadas | +400% ‚úÖ |
| **Monitoramento** | 2 processos | 5 processos granulares | +150% ‚úÖ |

**Observa√ß√£o importante**: Tempo total N√ÉO reduz porque Estado RJ (8h) √© gargalo inevit√°vel.

### Resili√™ncia (Principal Ganho)

**Cen√°rio: Falha no Estado RJ (p√°gina 1,500)**

**V1 (Atual)**:
```
Processo ESPECIAL falha na p√°gina 1,500 do Estado RJ (ap√≥s 6h)
‚ùå Perda: 6h de trabalho
‚ùå Re-execu√ß√£o: TODAS as 41 entidades do zero
‚ùå Tempo desperdi√ßado: 6h + 8h (re-run) = 14h total
```

**V2 (Paralelo)**:
```
Processo 1 falha na p√°gina 1,500 do Estado RJ (ap√≥s 6h)
‚úÖ Processos 2, 3, 4, 5 continuam normalmente
‚úÖ Ao final: 4 processos completos, 1 falhado
‚úÖ Re-execu√ß√£o: Apenas Estado RJ (rodar Processo 1 sozinho)
‚úÖ Tempo desperdi√ßado: 0h (outros completaram)
‚úÖ Tempo total: 8h (inicial) + 8h (re-run Estado RJ) = 16h
  vs 14h do V1, MAS com dados de 40 entidades j√° salvos
```

**Benef√≠cio**: Re-execu√ß√µes parciais poss√≠veis.

### Uso de Recursos

**V1 (2 processos)**:
```
RAM: ~640 MB
CPU: ~50% (em quad-core)
Aproveitamento: ‚ö†Ô∏è Subutilizado
```

**V2 (5 processos)**:
```
RAM: ~1,600 MB (11% de 16 GB)
CPU: ~100% (em quad-core)
Aproveitamento: ‚úÖ √ìtimo
```

---

## ‚ö†Ô∏è Riscos e Mitiga√ß√µes

### Risco 1: Rate Limiting do Site

**Problema**: 5 processos simult√¢neos podem acionar throttling

**Probabilidade**: 30% (n√£o testado al√©m de 2 processos)

**Mitiga√ß√£o**:
```python
# Adicionar delays randomizados entre requests
import random

delay = random.uniform(500, 1500)  # 0.5-1.5s
page.wait_for_timeout(delay)
```

**Plano B**: Reduzir de 5 ‚Üí 3 processos

---

### Risco 2: Consumo de RAM em VPS

**Problema**: 5 processos = ~1,600 MB (pode exceder em picos)

**Probabilidade**: 10% (VPS KVM 4 tem 16 GB)

**Mitiga√ß√£o**:
- Configurar 4 GB de swap
- Monitorar com `htop`
- Matar processos se RAM > 90%

---

### Risco 3: Conflito de Escrita em CSV

**Problema**: M√∫ltiplos processos escrevendo no mesmo arquivo

**Probabilidade**: 0% (design elimina risco)

**Solu√ß√£o no design**:
```
Cada processo escreve CSV pr√≥prio:
  - process_1_especial.csv
  - process_2_especial.csv
  - ...

Merge AP√ìS todos terminarem (sem concorr√™ncia)
```

---

### Risco 4: Browser Crashes

**Problema**: Playwright/Chromium pode crashar ap√≥s horas

**Probabilidade**: 5%

**Mitiga√ß√£o**:
```python
try:
    precatorios = extract_entities(entity_list)
except Exception as e:
    logger.error(f"Process crashed: {e}")
    # Salvar progresso parcial
    save_to_csv(precatorios, f"partial_{timestamp}.csv")
```

---

## üí∞ An√°lise Custo-Benef√≠cio

| Aspecto | Estimativa |
|---------|------------|
| **Implementa√ß√£o** | 2h |
| **Testes** | 1h |
| **Total Investimento** | **3h** |
| **Probabilidade Sucesso** | **100%** |
| **Ganho em Tempo** | 0h (mesmo 8h) |
| **Ganho em Resili√™ncia** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Alto |
| **Ganho em CPU** | +80% utiliza√ß√£o |
| **ROI** | +200% (resili√™ncia + otimiza√ß√£o) |

**Valor Esperado**:
```
100% √ó (Resili√™ncia + Otimiza√ß√£o) = Alto retorno
Investimento: 3h
ROI: Positivo (+200%)
```

**Conclus√£o**: **VALE A PENA** implementar esta estrat√©gia.

---

## üéØ Recomenda√ß√£o de Deployment

### Ambiente Ideal: VPS Hostinger KVM 4

**Especifica√ß√µes**:
- 4 vCPUs
- 16 GB RAM
- Ubuntu 22.04 LTS
- Playwright headless

**Comando de Execu√ß√£o**:
```bash
# Ativar ambiente virtual
source venv/bin/activate

# Executar em background com nohup
nohup python3 v2_parallel/main_parallel.py > logs/parallel_execution.log 2>&1 &

# Monitorar progresso
tail -f logs/parallel_execution.log

# Monitorar recursos
htop
```

**Monitoramento**:
```bash
# Ver processos Python ativos
ps aux | grep python

# Ver logs de cada processo
tail -f logs/process_1.log
tail -f logs/process_2.log
# ...

# Ver progresso em tempo real
watch -n 10 'wc -l data/partial/*.csv'
```

---

## üìã Checklist de Implementa√ß√£o

### Pr√©-Implementa√ß√£o
- [ ] Validar que V1 est√° funcional (100% campos extra√≠dos)
- [ ] Confirmar que bugs de campos expandidos foram corrigidos
- [ ] Backup dos CSVs atuais (V1)
- [ ] Provisionar VPS KVM 4 (ou preparar m√°quina local com 4+ cores)

### Implementa√ß√£o
- [ ] Criar estrutura `v2_parallel/` (30min)
- [ ] Refatorar `scraper.py` para aceitar lista de entidades (1h)
- [ ] Criar `config/entity_groups.py` com distribui√ß√£o (30min)
- [ ] Implementar `main_parallel.py` com multiprocessing (30min)
- [ ] Implementar merge de CSVs (30min)
- [ ] Criar testes unit√°rios (30min)

### Testes
- [ ] Teste local com 2 processos e 3 entidades pequenas (15min)
- [ ] Validar CSVs parciais gerados corretamente (10min)
- [ ] Validar merge de CSVs (10min)
- [ ] Teste completo local (se hardware permitir) ou em VPS (8h)

### Deployment VPS
- [ ] Deploy c√≥digo para VPS
- [ ] Instalar depend√™ncias (Python, Playwright, etc)
- [ ] Testar com 2 processos primeiro (smoke test)
- [ ] Executar com 5 processos
- [ ] Monitorar primeiras 2h de execu√ß√£o
- [ ] Validar logs e progresso

### P√≥s-Deployment
- [ ] Validar CSVs finais (contagem de registros)
- [ ] Verificar 100% cobertura de campos expandidos
- [ ] Comparar com V1 (baseline)
- [ ] Documentar issues encontrados
- [ ] Medir consumo de recursos (RAM, CPU)

---

## üîÑ Compara√ß√£o com Outras Estrat√©gias

| Crit√©rio | Op√ß√£o 1 (Atual) | Op√ß√£o 2 (API) | **Op√ß√£o 3 (Entidades)** ‚≠ê |
|----------|-----------------|---------------|----------------------------|
| **Tempo Total** | ~8h | ~2-3h (SE API existir) | ~8h |
| **Investimento** | 0h | 10-13h | **3h** ‚úÖ |
| **Probabilidade Sucesso** | 100% | ~20% | **100%** ‚úÖ |
| **Resili√™ncia** | Baixa | Alta (SE funcionar) | **Alta** ‚úÖ |
| **Uso de CPU** | 50% | 80-90% | **90-100%** ‚úÖ |
| **Risco Legal** | Zero | Alto (eng. reversa) | **Zero** ‚úÖ |
| **Escalabilidade** | Baixa | Alta (SE API existir) | **M√©dia** ‚úÖ |
| **Complexidade** | Muito Baixa | Muito Alta | **Baixa-M√©dia** ‚úÖ |
| **ROI** | N/A | -90% | **+200%** ‚úÖ |

**Veredicto**: Op√ß√£o 3 √© **claramente superior** em todos os crit√©rios exceto tempo total (que √© limitado pelo Estado RJ inevitavelmente).

---

## üöÄ Pr√≥ximos Passos

### Implementa√ß√£o Imediata (Se Aprovado)

1. **Criar estrutura de c√≥digo** (30min)
   ```bash
   mkdir -p v2_parallel/{config,extractors,utils,logs}
   touch v2_parallel/main_parallel.py
   ```

2. **Refatorar extrator** (1h)
   - Extrair l√≥gica para `extractors/entity_extractor.py`
   - Aceitar lista de entidades
   - Retornar CSV path

3. **Configurar grupos** (30min)
   - Carregar entidades de CSVs
   - Criar distribui√ß√£o balanceada
   - Validar 97 entidades

4. **Implementar orquestra√ß√£o** (30min)
   - `multiprocessing.Pool`
   - Spawnar 5 processos
   - Coletar resultados

5. **Implementar merge** (30min)
   - Ler CSVs parciais
   - Concatenar com Pandas
   - Validar colunas

6. **Testar** (1h)
   - Teste pequeno (2 processos, 3 entidades)
   - Teste completo (5 processos, 97 entidades)

**Tempo Total**: **3h** (implementa√ß√£o + testes)

---

## üìö Refer√™ncias

- `findings/01_api_investigation.md` - Por que Op√ß√£o 2 n√£o √© vi√°vel
- `findings/02_performance_analysis.md` - Gargalo do Estado RJ
- `findings/03_resource_requirements.md` - Capacidade de VPS KVM 4
- `findings/04_current_bugs_fixed.md` - Bugs j√° corrigidos (baseline V1)
- `strategies/option1_maintain_current.md` - Configura√ß√£o atual (baseline)
- `strategies/option2_api_ranges.md` - Alternativa n√£o recomendada

---

## üí° Melhorias Futuras (P√≥s-V2)

### Otimiza√ß√£o 1: Checkpoint/Resume por Entidade
```python
# Salvar progresso a cada 100 p√°ginas
if page_num % 100 == 0:
    save_checkpoint(entidade_id, page_num, precatorios)

# Retomar de onde parou
if checkpoint_exists(entidade_id):
    start_page = load_checkpoint(entidade_id)
```

**Ganho**: Resili√™ncia adicional (n√£o perde horas em caso de crash)

### Otimiza√ß√£o 2: Eliminar Collapse de Expandidos
```python
# N√£o clicar "-" ap√≥s extrair
# Deixar todos expandidos acumulados

# Ganho: ~30% redu√ß√£o de tempo (8h ‚Üí 5.6h)
# Risco: Poss√≠vel polui√ß√£o de DOM
```

### Otimiza√ß√£o 3: Dynamic Process Allocation
```python
# Quando Processo 4 termina (1h), reatribuir para ajudar Processo 1
# Dividir Estado RJ dinamicamente entre processos ociosos

# Requer: API de pagina√ß√£o (n√£o dispon√≠vel)
```

---

**√öltima Atualiza√ß√£o**: 2025-11-26
**Status**: ‚úÖ RECOMENDADO para implementa√ß√£o imediata
**Pr√≥ximo Passo**: Aguardar aprova√ß√£o do usu√°rio para come√ßar implementa√ß√£o

---

## ‚úÖ Resumo Executivo

**O Que Fazer**: Dividir 97 entidades em 5 grupos balanceados por tempo, cada um rodando em processo separado.

**Por Que Fazer**: Melhor uso de CPU (50% ‚Üí 100%), resili√™ncia alta, re-execu√ß√µes parciais poss√≠veis.

**Quanto Custa**: 3h de implementa√ß√£o + testes.

**Quanto Economiza**: 0h em tempo (Estado RJ limita), MAS ganho enorme em resili√™ncia e otimiza√ß√£o.

**Risco**: Muito baixo (~5% de rate limiting, mitig√°vel).

**Recomenda√ß√£o**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **IMPLEMENTAR IMEDIATAMENTE**
